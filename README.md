# Responsible AI Cases 🕵️‍♂️

This mini-project investigates two real-world scenarios where AI systems show signs of unfairness or ethical concerns.

## 👇 Included Cases

### 1. The Hiring Bot That Ghosted Women
- Scenario: AI screening system penalizes applicants with career gaps, disproportionately affecting women.
- Issue: Gender bias, lack of transparency.
- Fix: Train with diverse data and give feedback to rejected applicants.

### 2. The Proctoring AI That Cried "Cheater!"
- Scenario: AI flags students for cheating based on eye movement, disproportionately affecting neurodivergent students.
- Issue: Ableism, false positives.
- Fix: Use multimodal signals and always include human review.

## ✍️ Author
Emmanuel Ouma — AI Ethics Explorer & Data Analyst  
